{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "earthquake.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexeyphilippov/NN_Grid_search/blob/master/earthquake_VGG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "8WA5ssNHZwcb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "\n",
        "# !cp gdrive/'My Drive'/Python_for_colab/quake/wavelet_img/both.tar .\n",
        "# !tar -xf both.tar\n",
        "\n",
        "# !cp gdrive/'My Drive'/Python_for_colab/quake/wavelet_img/image_map.tar .\n",
        "# !tar -xf image_map.tar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ab8FhZEhGwOL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from random import shuffle\n",
        "import matplotlib.image as mpimg\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "if9yjLOtGwOV",
        "colab_type": "code",
        "outputId": "40c3eb5f-310c-47ec-e6b6-116bf74339ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fb0cc3d14b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "FiVyy3moGwOf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_capacity = 8040\n",
        "test_capacity = 1400\n",
        "root_for_imgs = 'Users/aleksejfilippov/Desktop/Python_projects/NN_for_Tanurkov/wavelet_img/both'\n",
        "root_for_csv = 'Users/aleksejfilippov/Desktop/Python_projects/NN_for_Tanurkov/wavelet_img/image_map.csv'\n",
        "class EarthQuakeDataset(Dataset):\n",
        "\n",
        "    def __init__(self, train:bool, both_path = None, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            pos_dir (string): Directory with all the images with earthquakes.\n",
        "            neg_dir (string): Directory with all the images without earthquakes.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.transform = transform\n",
        "          \n",
        "        self.map_ = pd.read_csv(root_for_csv, sep = 'Â±')\n",
        "                \n",
        "        if train:\n",
        "            self.img_paths = self.map_.path[:train_capacity]\n",
        "            self.labels = self.map_.label[:train_capacity]\n",
        "        elif train != True:\n",
        "            self.img_paths = self.map_.path[train_capacity:train_capacity+test_capacity].tolist()\n",
        "            self.labels = self.map_.label[train_capacity:train_capacity+test_capacity].tolist()\n",
        "        else:\n",
        "            raise Exception('Parameter train must be set bull')\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \n",
        "        image = mpimg.imread(os.path.join(root_for_imgs, self.img_paths[idx]))[:-3, :-3]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        return image, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L4kCzTdpGwOj",
        "colab_type": "code",
        "outputId": "62a12404-1311-4e51-dc76-d20a6572906e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,),(0.5,))])\n",
        "train_set = EarthQuakeDataset(train = True, transform = transform)\n",
        "test_set = EarthQuakeDataset(train = False, transform = transform)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: ParserWarning: Falling back to the 'python' engine because the separator encoded in utf-8 is > 1 char long, and the 'c' engine does not support such separators; you can avoid this warning by specifying engine='python'.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: ParserWarning: Falling back to the 'python' engine because the separator encoded in utf-8 is > 1 char long, and the 'c' engine does not support such separators; you can avoid this warning by specifying engine='python'.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "YAgnIY83GwOn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#DataLoader takes in a dataset and a sampler for loading (num_workers deals with system level memory) \n",
        "def get_train_loader(batch_size):\n",
        "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, num_workers=2)\n",
        "    return(train_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7oqfnGGOGwOp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Test loader has constant batch sizes, so we can define it directly\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=20, num_workers=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w1zdAmFAGwOs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def createLossAndOptimizer(net, learning_rate=0.001):\n",
        "    \n",
        "    #Loss function\n",
        "    loss = torch.nn.BCELoss()\n",
        "    \n",
        "    #Optimizer\n",
        "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "    \n",
        "    return(loss, optimizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JB-evv2Ugyo2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "class VGG(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG, self).__init__()\n",
        "        # 80x80 \n",
        "        self.conv1_1 = torch.nn.Conv2d(1,64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv1_2 = torch.nn.Conv2d(64,64, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        # 40x40\n",
        "        self.conv2_1 = torch.nn.Conv2d(64,128, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2_2 = torch.nn.Conv2d(128,128, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool2 = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        # 20x20\n",
        "        self.conv3_1 = torch.nn.Conv2d(128,256, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3_2 = torch.nn.Conv2d(256,256, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool3 = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        # 10x10\n",
        "        self.conv4_1 = torch.nn.Conv2d(256,512, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv4_2 = torch.nn.Conv2d(512,512, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool4 = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        # 5x5\n",
        "        self.fc1 = torch.nn.Linear(5 * 5 * 512, 5 * 5 * 512)\n",
        "        self.fc2 = torch.nn.Linear(5 * 5 * 512, 1)\n",
        "        \n",
        "        self.softmax = torch.nn.Softmax()\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.conv1_1(x)\n",
        "        x = self.conv1_2(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = self.conv2_1(x)\n",
        "        x = self.conv2_2(x)\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = self.conv3_1(x)\n",
        "        x = self.conv3_2(x)\n",
        "        x = self.pool3(x)\n",
        "\n",
        "        x = self.conv4_1(x)\n",
        "        x = self.conv4_2(x)\n",
        "        x = self.pool4(x)\n",
        "\n",
        "        x = x.view(-1, 5 * 5 * 512)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.softmax(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1GK4ZyphGwOv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SimpleCNN(torch.nn.Module):\n",
        "        \n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        \n",
        "        # 80x80 \n",
        "        self.conv1_1 = torch.nn.Conv2d(1,64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv1_2 = torch.nn.Conv2d(64,64, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        # 40x40\n",
        "        self.conv2_1 = torch.nn.Conv2d(64,128, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2_2 = torch.nn.Conv2d(128,128, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool2 = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        # 20x20\n",
        "        self.fc1 = torch.nn.Linear(128 * 20 * 20, 128)        \n",
        "        self.dropout1 = torch.nn.Dropout(p = 0.6)\n",
        "        self.fc2 = torch.nn.Linear(128, 2)\n",
        "        \n",
        "        self.softmax = torch.nn.Softmax()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.conv1_1(x)\n",
        "        x = self.conv1_2(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = self.conv2_1(x)\n",
        "        x = self.conv2_2(x)\n",
        "        x = self.pool2(x)\n",
        "        \n",
        "        x = x.view(-1, 128 * 20 * 20)\n",
        "        \n",
        "        x = F.relu(self.fc1(x))\n",
        "        \n",
        "        x = self.fc2(self.dropout1(x))\n",
        "        \n",
        "        x = self.softmax(x)\n",
        "        return(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4wlsSguT-Tfm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def ohe(inp:torch.tensor):\n",
        "    x = []\n",
        "    for v in inp:\n",
        "        if v == 1:\n",
        "            x.append([1,0])\n",
        "        elif v==0:\n",
        "            x.append([0,1])\n",
        "    return torch.tensor(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VqNVNQUDGwO9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "def trainNet(net, batch_size, n_epochs, learning_rate):\n",
        "    \n",
        "    #Print all of the hyperparameters of the training iteration:\n",
        "    print(\"===== HYPERPARAMETERS =====\")\n",
        "    print(\"batch_size=\", batch_size)\n",
        "    print(\"epochs=\", n_epochs)\n",
        "    print(\"learning_rate=\", learning_rate)\n",
        "    print(\"=\" * 30)\n",
        "    \n",
        "    #Get training data\n",
        "    train_loader = get_train_loader(batch_size)\n",
        "    n_batches = len(train_loader)\n",
        "    \n",
        "    #Create our loss and optimizer functions\n",
        "    loss, optimizer = createLossAndOptimizer(net, learning_rate)\n",
        "    \n",
        "    #Time for printing\n",
        "    training_start_time = time.time()\n",
        "    \n",
        "    #Loop for n_epochs\n",
        "    test_losses = []\n",
        "    train_losses = []\n",
        "    for epoch in range(n_epochs):\n",
        "        \n",
        "        running_loss = 0.0\n",
        "        print_every = n_batches // 10\n",
        "        start_time = time.time()\n",
        "        total_train_loss = 0\n",
        "        \n",
        "        for i, data in enumerate(train_loader):\n",
        "            \n",
        "            #Get inputs\n",
        "            inputs, labels = data\n",
        "            labels = ohe(labels)\n",
        "            inputs = inputs.view((inputs.shape[0],1, inputs.shape[2], inputs.shape[2]))\n",
        "            \n",
        "            #Wrap them in a Variable object\n",
        "            inputs, labels = Variable(inputs), Variable(labels.type(torch.FloatTensor))\n",
        "            \n",
        "            #Set the parameter gradients to zero\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            #Forward pass, backward pass, optimize\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = net(inputs)\n",
        "            labels = labels.to(device)\n",
        "            outputs = outputs.to(device)\n",
        "            loss_size = loss(outputs, labels)\n",
        "            loss_size.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            #Print statistics\n",
        "            running_loss += loss_size.data.item()\n",
        "            total_train_loss += loss_size.data.item()\n",
        "            \n",
        "            #Print every 10th batch of an epoch\n",
        "            if (i + 1) % (print_every + 1) == 0:\n",
        "                print(\"Epoch {}, {:d}% \\t train_loss: {:.2f} took: {:.2f}s\".format(\n",
        "                        epoch+1, int(100 * (i+1) / n_batches), running_loss / print_every, time.time() - start_time))\n",
        "                #Reset running loss and time\n",
        "                running_loss = 0.0\n",
        "                start_time = time.time()\n",
        "            \n",
        "        train_losses.append(total_train_loss/len(train_loader))\n",
        "        #At the end of the epoch, do a pass on the validation set\n",
        "        total_test_loss = 0\n",
        "        for inputs, labels in test_loader:\n",
        "            \n",
        "            labels = ohe(labels)\n",
        "            #Wrap tensors in Variables\n",
        "            inputs = inputs.view((inputs.shape[0],1, inputs.shape[2], inputs.shape[2]))\n",
        "            inputs, labels = Variable(inputs), Variable(labels.type(torch.FloatTensor))\n",
        "            \n",
        "            #Forward pass\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            test_outputs = net(inputs)\n",
        "            test_outputs = test_outputs.to(device)\n",
        "            test_loss_size = loss(test_outputs, labels)\n",
        "            total_test_loss += test_loss_size.data.item()\n",
        "            \n",
        "        print(\"Test loss = {:.2f}\".format(total_test_loss / len(test_loader)))\n",
        "        test_losses.append(total_test_loss / len(test_loader))\n",
        "        \n",
        "    print(\"Training finished, took {:.2f}s\".format(time.time() - training_start_time))\n",
        "    plt.plot(range(len(train_losses)), train_losses)\n",
        "    plt.plot(range(len(train_losses)), test_losses)\n",
        "    plt.show()\n",
        "    return train_losses, test_losses, optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pgJ34P6c-e8u",
        "colab_type": "code",
        "outputId": "4afa4a99-fd48-4fb8-892c-49cebcd4d80d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8582
        }
      },
      "cell_type": "code",
      "source": [
        "cnn = SimpleCNN()\n",
        "cnn.to(device)\n",
        "tr, te ,optimis = trainNet(cnn, batch_size=80, n_epochs=150, learning_rate=0.0001)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===== HYPERPARAMETERS =====\n",
            "batch_size= 80\n",
            "epochs= 150\n",
            "learning_rate= 0.0001\n",
            "==============================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, 10% \t train_loss: 0.72 took: 67.27s\n",
            "Epoch 1, 21% \t train_loss: 0.72 took: 65.49s\n",
            "Epoch 1, 32% \t train_loss: 0.69 took: 65.61s\n",
            "Epoch 1, 43% \t train_loss: 0.65 took: 65.44s\n",
            "Epoch 1, 54% \t train_loss: 0.61 took: 65.08s\n",
            "Epoch 1, 65% \t train_loss: 0.63 took: 65.28s\n",
            "Epoch 1, 76% \t train_loss: 0.60 took: 65.22s\n",
            "Epoch 1, 87% \t train_loss: 0.61 took: 65.55s\n",
            "Epoch 1, 98% \t train_loss: 0.57 took: 65.65s\n",
            "Test loss = 0.51\n",
            "Epoch 2, 10% \t train_loss: 0.55 took: 66.40s\n",
            "Epoch 2, 21% \t train_loss: 0.56 took: 65.26s\n",
            "Epoch 2, 32% \t train_loss: 0.55 took: 65.53s\n",
            "Epoch 2, 43% \t train_loss: 0.51 took: 65.65s\n",
            "Epoch 2, 54% \t train_loss: 0.50 took: 65.38s\n",
            "Epoch 2, 65% \t train_loss: 0.52 took: 65.46s\n",
            "Epoch 2, 76% \t train_loss: 0.52 took: 65.44s\n",
            "Epoch 2, 87% \t train_loss: 0.52 took: 70.57s\n",
            "Epoch 2, 98% \t train_loss: 0.49 took: 70.75s\n",
            "Test loss = 0.44\n",
            "Epoch 3, 10% \t train_loss: 0.47 took: 66.42s\n",
            "Epoch 3, 21% \t train_loss: 0.48 took: 65.27s\n",
            "Epoch 3, 32% \t train_loss: 0.47 took: 65.96s\n",
            "Epoch 3, 43% \t train_loss: 0.43 took: 65.46s\n",
            "Epoch 3, 54% \t train_loss: 0.44 took: 65.39s\n",
            "Epoch 3, 65% \t train_loss: 0.44 took: 65.33s\n",
            "Epoch 3, 76% \t train_loss: 0.44 took: 65.56s\n",
            "Epoch 3, 87% \t train_loss: 0.46 took: 66.06s\n",
            "Epoch 3, 98% \t train_loss: 0.44 took: 74.57s\n",
            "Test loss = 0.40\n",
            "Epoch 4, 10% \t train_loss: 0.41 took: 66.44s\n",
            "Epoch 4, 21% \t train_loss: 0.44 took: 70.24s\n",
            "Epoch 4, 32% \t train_loss: 0.41 took: 65.78s\n",
            "Epoch 4, 43% \t train_loss: 0.39 took: 65.43s\n",
            "Epoch 4, 54% \t train_loss: 0.41 took: 65.60s\n",
            "Epoch 4, 65% \t train_loss: 0.39 took: 65.52s\n",
            "Epoch 4, 76% \t train_loss: 0.41 took: 65.72s\n",
            "Epoch 4, 87% \t train_loss: 0.43 took: 65.81s\n",
            "Epoch 4, 98% \t train_loss: 0.45 took: 65.40s\n",
            "Test loss = 0.50\n",
            "Epoch 5, 10% \t train_loss: 0.61 took: 66.98s\n",
            "Epoch 5, 21% \t train_loss: 0.59 took: 66.00s\n",
            "Epoch 5, 32% \t train_loss: 0.53 took: 65.76s\n",
            "Epoch 5, 43% \t train_loss: 0.47 took: 65.69s\n",
            "Epoch 5, 54% \t train_loss: 0.49 took: 65.84s\n",
            "Epoch 5, 65% \t train_loss: 0.47 took: 65.99s\n",
            "Epoch 5, 76% \t train_loss: 0.45 took: 66.03s\n",
            "Epoch 5, 87% \t train_loss: 0.47 took: 65.73s\n",
            "Epoch 5, 98% \t train_loss: 0.46 took: 65.57s\n",
            "Test loss = 0.39\n",
            "Epoch 6, 10% \t train_loss: 0.42 took: 66.95s\n",
            "Epoch 6, 21% \t train_loss: 0.41 took: 65.96s\n",
            "Epoch 6, 32% \t train_loss: 0.38 took: 66.01s\n",
            "Epoch 6, 43% \t train_loss: 0.38 took: 65.86s\n",
            "Epoch 6, 54% \t train_loss: 0.41 took: 65.56s\n",
            "Epoch 6, 65% \t train_loss: 0.39 took: 65.87s\n",
            "Epoch 6, 76% \t train_loss: 0.39 took: 66.05s\n",
            "Epoch 6, 87% \t train_loss: 0.41 took: 66.10s\n",
            "Epoch 6, 98% \t train_loss: 0.35 took: 70.43s\n",
            "Test loss = 0.36\n",
            "Epoch 7, 10% \t train_loss: 0.39 took: 67.02s\n",
            "Epoch 7, 21% \t train_loss: 0.39 took: 66.35s\n",
            "Epoch 7, 32% \t train_loss: 0.39 took: 66.26s\n",
            "Epoch 7, 43% \t train_loss: 0.35 took: 65.85s\n",
            "Epoch 7, 54% \t train_loss: 0.37 took: 65.91s\n",
            "Epoch 7, 65% \t train_loss: 0.35 took: 66.26s\n",
            "Epoch 7, 76% \t train_loss: 0.36 took: 65.93s\n",
            "Epoch 7, 87% \t train_loss: 0.41 took: 65.72s\n",
            "Epoch 7, 98% \t train_loss: 0.36 took: 65.96s\n",
            "Test loss = 0.34\n",
            "Epoch 8, 10% \t train_loss: 0.39 took: 67.33s\n",
            "Epoch 8, 21% \t train_loss: 0.36 took: 66.39s\n",
            "Epoch 8, 32% \t train_loss: 0.36 took: 66.33s\n",
            "Epoch 8, 43% \t train_loss: 0.32 took: 66.59s\n",
            "Epoch 8, 54% \t train_loss: 0.36 took: 66.81s\n",
            "Epoch 8, 65% \t train_loss: 0.34 took: 66.06s\n",
            "Epoch 8, 76% \t train_loss: 0.33 took: 65.90s\n",
            "Epoch 8, 87% \t train_loss: 0.36 took: 65.77s\n",
            "Epoch 8, 98% \t train_loss: 0.34 took: 70.84s\n",
            "Test loss = 0.33\n",
            "Epoch 9, 10% \t train_loss: 0.36 took: 67.56s\n",
            "Epoch 9, 21% \t train_loss: 0.35 took: 71.17s\n",
            "Epoch 9, 32% \t train_loss: 0.35 took: 66.00s\n",
            "Epoch 9, 43% \t train_loss: 0.31 took: 71.44s\n",
            "Epoch 9, 54% \t train_loss: 0.36 took: 66.49s\n",
            "Epoch 9, 65% \t train_loss: 0.33 took: 66.84s\n",
            "Epoch 9, 76% \t train_loss: 0.34 took: 66.09s\n",
            "Epoch 9, 87% \t train_loss: 0.37 took: 65.81s\n",
            "Epoch 9, 98% \t train_loss: 0.34 took: 66.17s\n",
            "Test loss = 0.32\n",
            "Epoch 10, 10% \t train_loss: 0.38 took: 66.91s\n",
            "Epoch 10, 21% \t train_loss: 0.35 took: 65.64s\n",
            "Epoch 10, 32% \t train_loss: 0.35 took: 65.57s\n",
            "Epoch 10, 43% \t train_loss: 0.30 took: 66.37s\n",
            "Epoch 10, 54% \t train_loss: 0.36 took: 65.81s\n",
            "Epoch 10, 65% \t train_loss: 0.32 took: 70.99s\n",
            "Epoch 10, 76% \t train_loss: 0.36 took: 65.99s\n",
            "Epoch 10, 87% \t train_loss: 0.37 took: 65.84s\n",
            "Epoch 10, 98% \t train_loss: 0.33 took: 65.87s\n",
            "Test loss = 0.32\n",
            "Epoch 11, 10% \t train_loss: 0.34 took: 67.91s\n",
            "Epoch 11, 21% \t train_loss: 0.31 took: 66.09s\n",
            "Epoch 11, 32% \t train_loss: 0.32 took: 65.47s\n",
            "Epoch 11, 43% \t train_loss: 0.30 took: 70.25s\n",
            "Epoch 11, 54% \t train_loss: 0.34 took: 65.69s\n",
            "Epoch 11, 65% \t train_loss: 0.30 took: 65.42s\n",
            "Epoch 11, 76% \t train_loss: 0.31 took: 65.40s\n",
            "Epoch 11, 87% \t train_loss: 0.35 took: 65.61s\n",
            "Epoch 11, 98% \t train_loss: 0.34 took: 68.78s\n",
            "Test loss = 0.30\n",
            "Epoch 12, 10% \t train_loss: 0.34 took: 66.93s\n",
            "Epoch 12, 21% \t train_loss: 0.33 took: 65.82s\n",
            "Epoch 12, 32% \t train_loss: 0.32 took: 66.19s\n",
            "Epoch 12, 43% \t train_loss: 0.28 took: 65.85s\n",
            "Epoch 12, 54% \t train_loss: 0.32 took: 66.06s\n",
            "Epoch 12, 65% \t train_loss: 0.33 took: 65.76s\n",
            "Epoch 12, 76% \t train_loss: 0.32 took: 68.16s\n",
            "Epoch 12, 87% \t train_loss: 0.35 took: 70.84s\n",
            "Epoch 12, 98% \t train_loss: 0.32 took: 70.44s\n",
            "Test loss = 0.30\n",
            "Epoch 13, 10% \t train_loss: 0.34 took: 66.50s\n",
            "Epoch 13, 21% \t train_loss: 0.33 took: 65.85s\n",
            "Epoch 13, 32% \t train_loss: 0.33 took: 65.64s\n",
            "Epoch 13, 43% \t train_loss: 0.30 took: 65.75s\n",
            "Epoch 13, 54% \t train_loss: 0.32 took: 65.92s\n",
            "Epoch 13, 65% \t train_loss: 0.33 took: 65.43s\n",
            "Epoch 13, 76% \t train_loss: 0.29 took: 66.29s\n",
            "Epoch 13, 87% \t train_loss: 0.34 took: 65.84s\n",
            "Epoch 13, 98% \t train_loss: 0.31 took: 70.98s\n",
            "Test loss = 0.30\n",
            "Epoch 14, 10% \t train_loss: 0.34 took: 66.70s\n",
            "Epoch 14, 21% \t train_loss: 0.31 took: 65.78s\n",
            "Epoch 14, 32% \t train_loss: 0.29 took: 65.97s\n",
            "Epoch 14, 43% \t train_loss: 0.27 took: 66.44s\n",
            "Epoch 14, 54% \t train_loss: 0.30 took: 65.89s\n",
            "Epoch 14, 65% \t train_loss: 0.29 took: 67.36s\n",
            "Epoch 14, 76% \t train_loss: 0.30 took: 66.14s\n",
            "Epoch 14, 87% \t train_loss: 0.33 took: 66.10s\n",
            "Epoch 14, 98% \t train_loss: 0.29 took: 68.39s\n",
            "Test loss = 0.29\n",
            "Epoch 15, 10% \t train_loss: 0.32 took: 67.51s\n",
            "Epoch 15, 21% \t train_loss: 0.30 took: 70.65s\n",
            "Epoch 15, 32% \t train_loss: 0.35 took: 66.22s\n",
            "Epoch 15, 43% \t train_loss: 0.32 took: 66.81s\n",
            "Epoch 15, 54% \t train_loss: 0.32 took: 66.27s\n",
            "Epoch 15, 65% \t train_loss: 0.31 took: 66.51s\n",
            "Epoch 15, 76% \t train_loss: 0.32 took: 66.09s\n",
            "Epoch 15, 87% \t train_loss: 0.32 took: 65.79s\n",
            "Epoch 15, 98% \t train_loss: 0.31 took: 70.98s\n",
            "Test loss = 0.28\n",
            "Epoch 16, 10% \t train_loss: 0.31 took: 67.03s\n",
            "Epoch 16, 21% \t train_loss: 0.29 took: 66.47s\n",
            "Epoch 16, 32% \t train_loss: 0.30 took: 66.02s\n",
            "Epoch 16, 43% \t train_loss: 0.28 took: 70.60s\n",
            "Epoch 16, 54% \t train_loss: 0.32 took: 65.86s\n",
            "Epoch 16, 65% \t train_loss: 0.29 took: 75.77s\n",
            "Epoch 16, 76% \t train_loss: 0.31 took: 65.87s\n",
            "Epoch 16, 87% \t train_loss: 0.32 took: 66.38s\n",
            "Epoch 16, 98% \t train_loss: 0.28 took: 66.00s\n",
            "Test loss = 0.30\n",
            "Epoch 17, 10% \t train_loss: 0.31 took: 66.95s\n",
            "Epoch 17, 21% \t train_loss: 0.29 took: 65.64s\n",
            "Epoch 17, 32% \t train_loss: 0.32 took: 65.80s\n",
            "Epoch 17, 43% \t train_loss: 0.26 took: 65.52s\n",
            "Epoch 17, 54% \t train_loss: 0.31 took: 65.78s\n",
            "Epoch 17, 65% \t train_loss: 0.28 took: 67.45s\n",
            "Epoch 17, 76% \t train_loss: 0.26 took: 65.57s\n",
            "Epoch 17, 87% \t train_loss: 0.34 took: 65.88s\n",
            "Epoch 17, 98% \t train_loss: 0.30 took: 66.15s\n",
            "Test loss = 0.28\n",
            "Epoch 18, 10% \t train_loss: 0.29 took: 67.35s\n",
            "Epoch 18, 21% \t train_loss: 0.31 took: 66.61s\n",
            "Epoch 18, 32% \t train_loss: 0.30 took: 66.83s\n",
            "Epoch 18, 43% \t train_loss: 0.26 took: 69.79s\n",
            "Epoch 18, 54% \t train_loss: 0.31 took: 66.74s\n",
            "Epoch 18, 65% \t train_loss: 0.26 took: 66.24s\n",
            "Epoch 18, 76% \t train_loss: 0.29 took: 66.55s\n",
            "Epoch 18, 87% \t train_loss: 0.32 took: 70.86s\n",
            "Epoch 18, 98% \t train_loss: 0.29 took: 75.26s\n",
            "Test loss = 0.28\n",
            "Epoch 19, 10% \t train_loss: 0.31 took: 67.51s\n",
            "Epoch 19, 21% \t train_loss: 0.30 took: 71.39s\n",
            "Epoch 19, 32% \t train_loss: 0.30 took: 66.33s\n",
            "Epoch 19, 43% \t train_loss: 0.26 took: 71.24s\n",
            "Epoch 19, 54% \t train_loss: 0.30 took: 66.18s\n",
            "Epoch 19, 65% \t train_loss: 0.29 took: 66.15s\n",
            "Epoch 19, 76% \t train_loss: 0.27 took: 66.26s\n",
            "Epoch 19, 87% \t train_loss: 0.32 took: 65.64s\n",
            "Epoch 19, 98% \t train_loss: 0.29 took: 70.58s\n",
            "Test loss = 0.27\n",
            "Epoch 20, 10% \t train_loss: 0.31 took: 67.33s\n",
            "Epoch 20, 21% \t train_loss: 0.30 took: 66.60s\n",
            "Epoch 20, 32% \t train_loss: 0.28 took: 66.91s\n",
            "Epoch 20, 43% \t train_loss: 0.24 took: 66.91s\n",
            "Epoch 20, 54% \t train_loss: 0.29 took: 67.00s\n",
            "Epoch 20, 65% \t train_loss: 0.26 took: 66.62s\n",
            "Epoch 20, 76% \t train_loss: 0.25 took: 68.89s\n",
            "Epoch 20, 87% \t train_loss: 0.32 took: 66.28s\n",
            "Epoch 20, 98% \t train_loss: 0.28 took: 69.93s\n",
            "Test loss = 0.29\n",
            "Epoch 21, 10% \t train_loss: 0.31 took: 67.52s\n",
            "Epoch 21, 21% \t train_loss: 0.29 took: 66.13s\n",
            "Epoch 21, 32% \t train_loss: 0.29 took: 66.20s\n",
            "Epoch 21, 43% \t train_loss: 0.22 took: 72.61s\n",
            "Epoch 21, 54% \t train_loss: 0.30 took: 65.78s\n",
            "Epoch 21, 65% \t train_loss: 0.29 took: 66.28s\n",
            "Epoch 21, 76% \t train_loss: 0.27 took: 66.29s\n",
            "Epoch 21, 87% \t train_loss: 0.31 took: 70.80s\n",
            "Epoch 21, 98% \t train_loss: 0.28 took: 70.71s\n",
            "Test loss = 0.27\n",
            "Epoch 22, 10% \t train_loss: 0.29 took: 67.83s\n",
            "Epoch 22, 21% \t train_loss: 0.29 took: 66.31s\n",
            "Epoch 22, 32% \t train_loss: 0.44 took: 65.91s\n",
            "Epoch 22, 43% \t train_loss: 0.86 took: 66.11s\n",
            "Epoch 22, 54% \t train_loss: 0.62 took: 65.58s\n",
            "Epoch 22, 65% \t train_loss: 0.62 took: 66.94s\n",
            "Epoch 22, 76% \t train_loss: 0.59 took: 66.54s\n",
            "Epoch 22, 87% \t train_loss: 0.56 took: 65.94s\n",
            "Epoch 22, 98% \t train_loss: 0.48 took: 65.96s\n",
            "Test loss = 0.40\n",
            "Epoch 23, 10% \t train_loss: 0.42 took: 67.36s\n",
            "Epoch 23, 21% \t train_loss: 0.39 took: 66.09s\n",
            "Epoch 23, 32% \t train_loss: 0.36 took: 65.69s\n",
            "Epoch 23, 43% \t train_loss: 0.31 took: 66.02s\n",
            "Epoch 23, 54% \t train_loss: 0.34 took: 65.99s\n",
            "Epoch 23, 65% \t train_loss: 0.30 took: 65.60s\n",
            "Epoch 23, 76% \t train_loss: 0.31 took: 65.72s\n",
            "Epoch 23, 87% \t train_loss: 0.36 took: 65.64s\n",
            "Epoch 23, 98% \t train_loss: 0.31 took: 65.74s\n",
            "Test loss = 0.30\n",
            "Epoch 24, 10% \t train_loss: 0.35 took: 66.72s\n",
            "Epoch 24, 21% \t train_loss: 0.34 took: 65.94s\n",
            "Epoch 24, 32% \t train_loss: 0.32 took: 65.99s\n",
            "Epoch 24, 43% \t train_loss: 0.29 took: 65.82s\n",
            "Epoch 24, 54% \t train_loss: 0.34 took: 66.78s\n",
            "Epoch 24, 65% \t train_loss: 0.32 took: 66.19s\n",
            "Epoch 24, 76% \t train_loss: 0.29 took: 66.12s\n",
            "Epoch 24, 87% \t train_loss: 0.34 took: 65.73s\n",
            "Epoch 24, 98% \t train_loss: 0.29 took: 66.23s\n",
            "Test loss = 0.30\n",
            "Epoch 25, 10% \t train_loss: 0.31 took: 66.95s\n",
            "Epoch 25, 21% \t train_loss: 0.33 took: 65.56s\n",
            "Epoch 25, 32% \t train_loss: 0.30 took: 65.54s\n",
            "Epoch 25, 43% \t train_loss: 0.26 took: 68.75s\n",
            "Epoch 25, 54% \t train_loss: 0.30 took: 66.37s\n",
            "Epoch 25, 65% \t train_loss: 0.29 took: 66.04s\n",
            "Epoch 25, 76% \t train_loss: 0.26 took: 65.68s\n",
            "Epoch 25, 87% \t train_loss: 0.32 took: 65.74s\n",
            "Epoch 25, 98% \t train_loss: 0.27 took: 66.04s\n",
            "Test loss = 0.29\n",
            "Epoch 26, 10% \t train_loss: 0.31 took: 67.02s\n",
            "Epoch 26, 21% \t train_loss: 0.29 took: 65.89s\n",
            "Epoch 26, 32% \t train_loss: 0.27 took: 65.66s\n",
            "Epoch 26, 43% \t train_loss: 0.25 took: 65.92s\n",
            "Epoch 26, 54% \t train_loss: 0.31 took: 66.68s\n",
            "Epoch 26, 65% \t train_loss: 0.28 took: 66.36s\n",
            "Epoch 26, 76% \t train_loss: 0.26 took: 66.30s\n",
            "Epoch 26, 87% \t train_loss: 0.30 took: 66.49s\n",
            "Epoch 26, 98% \t train_loss: 0.28 took: 66.16s\n",
            "Test loss = 0.27\n",
            "Epoch 27, 10% \t train_loss: 0.29 took: 66.66s\n",
            "Epoch 27, 21% \t train_loss: 0.29 took: 65.49s\n",
            "Epoch 27, 32% \t train_loss: 0.28 took: 65.47s\n",
            "Epoch 27, 43% \t train_loss: 0.25 took: 65.98s\n",
            "Epoch 27, 54% \t train_loss: 0.29 took: 65.68s\n",
            "Epoch 27, 65% \t train_loss: 0.27 took: 70.31s\n",
            "Epoch 27, 76% \t train_loss: 0.26 took: 70.67s\n",
            "Epoch 27, 87% \t train_loss: 0.31 took: 70.79s\n",
            "Epoch 27, 98% \t train_loss: 0.27 took: 71.18s\n",
            "Test loss = 0.25\n",
            "Epoch 28, 10% \t train_loss: 0.29 took: 67.19s\n",
            "Epoch 28, 21% \t train_loss: 0.29 took: 65.67s\n",
            "Epoch 28, 32% \t train_loss: 0.27 took: 66.74s\n",
            "Epoch 28, 43% \t train_loss: 0.24 took: 71.06s\n",
            "Epoch 28, 54% \t train_loss: 0.28 took: 65.95s\n",
            "Epoch 28, 65% \t train_loss: 0.28 took: 70.51s\n",
            "Epoch 28, 76% \t train_loss: 0.26 took: 65.85s\n",
            "Epoch 28, 87% \t train_loss: 0.31 took: 66.28s\n",
            "Epoch 28, 98% \t train_loss: 0.29 took: 71.05s\n",
            "Test loss = 0.25\n",
            "Epoch 29, 10% \t train_loss: 0.29 took: 71.46s\n",
            "Epoch 29, 21% \t train_loss: 0.26 took: 65.93s\n",
            "Epoch 29, 32% \t train_loss: 0.30 took: 65.93s\n",
            "Epoch 29, 43% \t train_loss: 0.25 took: 69.47s\n",
            "Epoch 29, 54% \t train_loss: 0.33 took: 65.67s\n",
            "Epoch 29, 65% \t train_loss: 0.29 took: 65.58s\n",
            "Epoch 29, 76% \t train_loss: 0.29 took: 65.59s\n",
            "Epoch 29, 87% \t train_loss: 0.30 took: 70.11s\n",
            "Epoch 29, 98% \t train_loss: 0.26 took: 68.35s\n",
            "Test loss = 0.26\n",
            "Epoch 30, 10% \t train_loss: 0.29 took: 66.74s\n",
            "Epoch 30, 21% \t train_loss: 0.28 took: 66.28s\n",
            "Epoch 30, 32% \t train_loss: 0.29 took: 70.68s\n",
            "Epoch 30, 43% \t train_loss: 0.60 took: 76.17s\n",
            "Epoch 30, 54% \t train_loss: 0.56 took: 73.06s\n",
            "Epoch 30, 65% \t train_loss: 0.59 took: 69.72s\n",
            "Epoch 30, 76% \t train_loss: 0.66 took: 76.83s\n",
            "Epoch 30, 87% \t train_loss: 0.63 took: 65.47s\n",
            "Epoch 30, 98% \t train_loss: 0.65 took: 68.57s\n",
            "Test loss = 0.53\n",
            "Epoch 31, 10% \t train_loss: 0.63 took: 66.59s\n",
            "Epoch 31, 21% \t train_loss: 0.63 took: 66.39s\n",
            "Epoch 31, 32% \t train_loss: 0.64 took: 65.57s\n",
            "Epoch 31, 43% \t train_loss: 0.57 took: 65.60s\n",
            "Epoch 31, 54% \t train_loss: 0.57 took: 65.52s\n",
            "Epoch 31, 65% \t train_loss: 0.56 took: 65.65s\n",
            "Epoch 31, 76% \t train_loss: 0.52 took: 66.18s\n",
            "Epoch 31, 87% \t train_loss: 0.61 took: 65.59s\n",
            "Epoch 31, 98% \t train_loss: 0.52 took: 70.53s\n",
            "Test loss = 0.50\n",
            "Epoch 32, 10% \t train_loss: 0.54 took: 70.62s\n",
            "Epoch 32, 21% \t train_loss: 0.53 took: 70.61s\n",
            "Epoch 32, 32% \t train_loss: 0.60 took: 65.65s\n",
            "Epoch 32, 43% \t train_loss: 0.48 took: 65.58s\n",
            "Epoch 32, 54% \t train_loss: 0.49 took: 65.51s\n",
            "Epoch 32, 65% \t train_loss: 0.48 took: 66.75s\n",
            "Epoch 32, 76% \t train_loss: 0.53 took: 68.28s\n",
            "Epoch 32, 87% \t train_loss: 0.56 took: 74.10s\n",
            "Epoch 32, 98% \t train_loss: 0.53 took: 70.26s\n",
            "Test loss = 0.47\n",
            "Epoch 33, 10% \t train_loss: 0.55 took: 67.88s\n",
            "Epoch 33, 21% \t train_loss: 0.54 took: 65.55s\n",
            "Epoch 33, 32% \t train_loss: 0.51 took: 73.49s\n",
            "Epoch 33, 43% \t train_loss: 0.47 took: 65.60s\n",
            "Epoch 33, 54% \t train_loss: 0.51 took: 69.85s\n",
            "Epoch 33, 65% \t train_loss: 0.51 took: 66.47s\n",
            "Epoch 33, 76% \t train_loss: 0.50 took: 73.72s\n",
            "Epoch 33, 87% \t train_loss: 0.51 took: 65.52s\n",
            "Epoch 33, 98% \t train_loss: 0.48 took: 65.71s\n",
            "Test loss = 0.48\n",
            "Epoch 34, 10% \t train_loss: 0.52 took: 67.37s\n",
            "Epoch 34, 21% \t train_loss: 0.51 took: 71.96s\n",
            "Epoch 34, 32% \t train_loss: 0.50 took: 67.34s\n",
            "Epoch 34, 43% \t train_loss: 0.50 took: 65.47s\n",
            "Epoch 34, 54% \t train_loss: 0.50 took: 66.41s\n",
            "Epoch 34, 65% \t train_loss: 0.51 took: 69.93s\n",
            "Epoch 34, 76% \t train_loss: 0.49 took: 76.68s\n",
            "Epoch 34, 87% \t train_loss: 0.50 took: 73.46s\n",
            "Epoch 34, 98% \t train_loss: 0.52 took: 74.15s\n",
            "Test loss = 0.43\n",
            "Epoch 35, 10% \t train_loss: 0.65 took: 66.84s\n",
            "Epoch 35, 21% \t train_loss: 0.72 took: 65.60s\n",
            "Epoch 35, 32% \t train_loss: 0.66 took: 65.59s\n",
            "Epoch 35, 43% \t train_loss: 0.60 took: 66.34s\n",
            "Epoch 35, 54% \t train_loss: 0.60 took: 66.17s\n",
            "Epoch 35, 65% \t train_loss: 0.64 took: 70.15s\n",
            "Epoch 35, 76% \t train_loss: 0.58 took: 69.92s\n",
            "Epoch 35, 87% \t train_loss: 0.57 took: 65.84s\n",
            "Epoch 35, 98% \t train_loss: 0.54 took: 69.68s\n",
            "Test loss = 0.48\n",
            "Epoch 36, 10% \t train_loss: 0.57 took: 74.55s\n",
            "Epoch 36, 21% \t train_loss: 0.59 took: 65.53s\n",
            "Epoch 36, 32% \t train_loss: 0.62 took: 65.85s\n",
            "Epoch 36, 43% \t train_loss: 0.53 took: 66.86s\n",
            "Epoch 36, 54% \t train_loss: 0.55 took: 66.15s\n",
            "Epoch 36, 65% \t train_loss: 0.55 took: 66.15s\n",
            "Epoch 36, 76% \t train_loss: 0.61 took: 66.00s\n",
            "Epoch 36, 87% \t train_loss: 0.54 took: 66.70s\n",
            "Epoch 36, 98% \t train_loss: 0.51 took: 73.68s\n",
            "Test loss = 0.48\n",
            "Epoch 37, 10% \t train_loss: 0.54 took: 74.96s\n",
            "Epoch 37, 21% \t train_loss: 0.55 took: 69.37s\n",
            "Epoch 37, 32% \t train_loss: 0.52 took: 67.55s\n",
            "Epoch 37, 43% \t train_loss: 0.51 took: 67.09s\n",
            "Epoch 37, 54% \t train_loss: 0.49 took: 67.26s\n",
            "Epoch 37, 65% \t train_loss: 0.51 took: 79.50s\n",
            "Epoch 37, 76% \t train_loss: 0.52 took: 67.81s\n",
            "Epoch 37, 87% \t train_loss: 0.54 took: 67.80s\n",
            "Epoch 37, 98% \t train_loss: 0.52 took: 67.64s\n",
            "Test loss = 0.48\n",
            "Epoch 38, 10% \t train_loss: 0.54 took: 68.63s\n",
            "Epoch 38, 21% \t train_loss: 0.49 took: 72.24s\n",
            "Epoch 38, 32% \t train_loss: 0.56 took: 68.13s\n",
            "Epoch 38, 43% \t train_loss: 0.60 took: 67.61s\n",
            "Epoch 38, 54% \t train_loss: 0.56 took: 67.49s\n",
            "Epoch 38, 65% \t train_loss: 0.52 took: 72.60s\n",
            "Epoch 38, 76% \t train_loss: 0.48 took: 73.43s\n",
            "Epoch 38, 87% \t train_loss: 0.58 took: 67.35s\n",
            "Epoch 38, 98% \t train_loss: 0.48 took: 67.29s\n",
            "Test loss = 0.46\n",
            "Epoch 39, 10% \t train_loss: 0.51 took: 74.54s\n",
            "Epoch 39, 21% \t train_loss: 0.51 took: 83.21s\n",
            "Epoch 39, 32% \t train_loss: 0.50 took: 74.74s\n",
            "Epoch 39, 43% \t train_loss: 1.03 took: 70.74s\n",
            "Epoch 39, 54% \t train_loss: 1.26 took: 67.22s\n",
            "Epoch 39, 65% \t train_loss: 3.40 took: 93.15s\n",
            "Epoch 39, 76% \t train_loss: 3.27 took: 83.17s\n",
            "Epoch 39, 87% \t train_loss: 2.52 took: 66.56s\n",
            "Epoch 39, 98% \t train_loss: 2.36 took: 66.67s\n",
            "Test loss = 2.10\n",
            "Epoch 40, 10% \t train_loss: 2.09 took: 68.22s\n",
            "Epoch 40, 21% \t train_loss: 2.47 took: 67.58s\n",
            "Epoch 40, 32% \t train_loss: 2.29 took: 67.55s\n",
            "Epoch 40, 43% \t train_loss: 2.64 took: 67.35s\n",
            "Epoch 40, 54% \t train_loss: 2.15 took: 67.29s\n",
            "Epoch 40, 65% \t train_loss: 2.60 took: 68.08s\n",
            "Epoch 40, 76% \t train_loss: 1.93 took: 67.77s\n",
            "Epoch 40, 87% \t train_loss: 2.83 took: 67.05s\n",
            "Epoch 40, 98% \t train_loss: 2.58 took: 67.28s\n",
            "Test loss = 2.53\n",
            "Epoch 41, 10% \t train_loss: 2.19 took: 68.82s\n",
            "Epoch 41, 21% \t train_loss: 2.61 took: 67.81s\n",
            "Epoch 41, 32% \t train_loss: 2.43 took: 68.34s\n",
            "Epoch 41, 43% \t train_loss: 2.27 took: 67.54s\n",
            "Epoch 41, 54% \t train_loss: 1.78 took: 68.12s\n",
            "Epoch 41, 65% \t train_loss: 2.47 took: 67.40s\n",
            "Epoch 41, 76% \t train_loss: 2.45 took: 67.58s\n",
            "Epoch 41, 87% \t train_loss: 2.47 took: 67.60s\n",
            "Epoch 41, 98% \t train_loss: 2.09 took: 68.01s\n",
            "Test loss = 2.04\n",
            "Epoch 42, 10% \t train_loss: 2.45 took: 69.30s\n",
            "Epoch 42, 21% \t train_loss: 2.47 took: 68.59s\n",
            "Epoch 42, 32% \t train_loss: 2.06 took: 68.24s\n",
            "Epoch 42, 43% \t train_loss: 1.88 took: 69.06s\n",
            "Epoch 42, 54% \t train_loss: 2.22 took: 68.20s\n",
            "Epoch 42, 65% \t train_loss: 2.50 took: 68.22s\n",
            "Epoch 42, 76% \t train_loss: 2.22 took: 67.94s\n",
            "Epoch 42, 87% \t train_loss: 2.42 took: 67.93s\n",
            "Epoch 42, 98% \t train_loss: 2.25 took: 68.69s\n",
            "Test loss = 2.22\n",
            "Epoch 43, 10% \t train_loss: 2.39 took: 68.81s\n",
            "Epoch 43, 21% \t train_loss: 2.71 took: 67.69s\n",
            "Epoch 43, 32% \t train_loss: 2.40 took: 67.50s\n",
            "Epoch 43, 43% \t train_loss: 2.43 took: 67.71s\n",
            "Epoch 43, 54% \t train_loss: 2.40 took: 67.89s\n",
            "Epoch 43, 65% \t train_loss: 2.32 took: 67.86s\n",
            "Epoch 43, 76% \t train_loss: 2.28 took: 68.26s\n",
            "Epoch 43, 87% \t train_loss: 2.28 took: 68.40s\n",
            "Epoch 43, 98% \t train_loss: 2.40 took: 68.43s\n",
            "Test loss = 2.01\n",
            "Epoch 44, 10% \t train_loss: 1.93 took: 68.93s\n",
            "Epoch 44, 21% \t train_loss: 2.59 took: 68.11s\n",
            "Epoch 44, 32% \t train_loss: 2.29 took: 68.71s\n",
            "Epoch 44, 43% \t train_loss: 2.25 took: 67.65s\n",
            "Epoch 44, 54% \t train_loss: 2.31 took: 68.17s\n",
            "Epoch 44, 65% \t train_loss: 2.40 took: 68.48s\n",
            "Epoch 44, 76% \t train_loss: 2.59 took: 68.38s\n",
            "Epoch 44, 87% \t train_loss: 2.20 took: 68.41s\n",
            "Epoch 44, 98% \t train_loss: 2.24 took: 67.98s\n",
            "Test loss = 2.08\n",
            "Epoch 45, 10% \t train_loss: 2.66 took: 69.28s\n",
            "Epoch 45, 21% \t train_loss: 2.41 took: 68.32s\n",
            "Epoch 45, 32% \t train_loss: 2.46 took: 68.55s\n",
            "Epoch 45, 43% \t train_loss: 2.22 took: 68.56s\n",
            "Epoch 45, 54% \t train_loss: 2.59 took: 68.70s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-91ede6e26675>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mte\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0moptimis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-ad997b2703d1>\u001b[0m in \u001b[0;36mtrainNet\u001b[0;34m(net, batch_size, n_epochs, learning_rate)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mloss_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mloss_size\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "KOcjV_CDGwPA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# vgg = VGG()\n",
        "# vgg.to(device)\n",
        "# tr, te ,optimis = trainNet(vgg, batch_size=80, n_epochs=150, learning_rate=0.0001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A-6utAabLlqc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_save_name = 'cnn.pt'\n",
        "path = F\"/content/gdrive/My Drive/Python_for_colab/quake/models/{model_save_name}\" \n",
        "torch.save(cnn.state_dict(), path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PxALSLigVF_9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}